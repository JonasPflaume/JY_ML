{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The benchmark of SARCOS dataset\n",
    "\n",
    "The performance of different algorithms on this dataset can be found in: https://github.com/Kaixhin/SARCOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with fourier(polynomial()) feature: 2.111,  \n",
    "NN feature not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jylearn.data.reg_data import robot_inv_data\n",
    "import numpy as np\n",
    "import torch as th\n",
    "device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "\n",
    "Loss = th.nn.MSELoss()\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = robot_inv_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.concatenate([X_train, X_val], axis=0), np.concatenate([Y_train, Y_val], axis=0)\n",
    "from jylearn.feature.polynomial import PolynomialFT\n",
    "from jylearn.feature.bellcurve import BellCurve\n",
    "from jylearn.feature.fourier import FourierBases\n",
    "from jylearn.parametric.ridge import RidgeReg\n",
    "\n",
    "param1 = np.linspace(0.05, 0.99, 7)\n",
    "param2 = np.arange(1, 7)\n",
    "\n",
    "f1 = FourierBases(param1, param2)\n",
    "f2 = PolynomialFT(2)\n",
    "\n",
    "X_f = f1(f2(X))\n",
    "X_test_f = f1(f2(X_test))\n",
    "print(\"feature dim: \", X_f.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_f_t, Y_t, X_test_f_t, Y_t_test = th.from_numpy(X_f).to(device), th.from_numpy(Y).to(device), th.from_numpy(X_test_f).to(device), th.from_numpy(Y_test).to(device)\n",
    "rr = RidgeReg().fit(X_f_t, Y_t)\n",
    "pred = rr.predict(X_test_f_t)\n",
    "mse = Loss(pred, Y_t_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test =\\\n",
    "    th.from_numpy(X_train).to(device).double(), th.from_numpy(Y_train).to(device).double(), th.from_numpy(X_val).to(device).double(), \\\n",
    "    th.from_numpy(Y_val).to(device).double(), th.from_numpy(X_test).to(device).double(), th.from_numpy(Y_test).to(device).double()\n",
    "    \n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from jylearn.parametric.mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network: 1.469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"layer\":4, \"nodes\":[21, 500, 500, 7], \"batch\":128, \"lr\":1e-3, \"decay\":0.}\n",
    "net = MLP(param).to(device)\n",
    "print(net)\n",
    "Loss = MSELoss()\n",
    "parameters = MLP.setParams(net, param[\"decay\"])\n",
    "optimizer = Adam(parameters, lr=param[\"lr\"])\n",
    "for _ in range(100):\n",
    "    for i in range(len(X_train)//param[\"batch\"]):\n",
    "        optimizer.zero_grad()\n",
    "        index = th.randperm(len(X_train))\n",
    "        curr_index = index[i*param[\"batch\"]:(i+1)*param[\"batch\"]]\n",
    "        X_b = X_train[curr_index]\n",
    "        Y_b = Y_train[curr_index]\n",
    "        pred = net(X_b)\n",
    "        L = Loss(pred, Y_b)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    with th.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        L_val = Loss(pred_val, Y_val)\n",
    "        print(\"Curr validation loss: \", L_val)\n",
    "net.eval()\n",
    "pred_test = net(X_test)\n",
    "print(Loss(pred_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsample GPR with hyperparameters optimization: 2.887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jylearn.nonparametric.gpr import ExactGPR\n",
    "from jylearn.kernel.kernels import White, RQK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = X_test.double(), Y_test.double()\n",
    "perm = th.randperm(X_train.size(0))\n",
    "idx = perm[:5000]\n",
    "X_train_, Y_train_ = X_train[idx].double(), Y_train[idx].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.ones((21,7)) * 30.\n",
    "sigma = np.ones((7,)) * 1.\n",
    "c = np.ones((7,)) * 1.\n",
    "\n",
    "kernel = White(c=c, dim_in=21, dim_out=7) + RQK(l=l, sigma=sigma, alpha=sigma, dim_in=21, dim_out=7)\n",
    "gpr = ExactGPR(kernel=kernel)\n",
    "gpr.fit(X_train_, Y_train_, call_hyper_opt=True, lr=0.05, epoch=9000, optimizer_type=\"RMSPROP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gpr.predict(X_test)\n",
    "print(Loss(pred, Y_test))\n",
    "\n",
    "del gpr, pred, kernel\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titsias' variational EM GPR: not working well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jylearn.nonparametric.vigpr import VariationalEMSparseGPR\n",
    "from jylearn.kernel.kernels import White, RBF\n",
    "\n",
    "l = np.ones([21, 7]) * 1.\n",
    "sigma = np.ones([7,]) * 1.\n",
    "c = np.ones([7,]) * 0.1\n",
    "\n",
    "white_kernel = White(c=c, dim_in=21, dim_out=7)\n",
    "kernel = RBF(sigma=sigma, l=l, dim_in=21, dim_out=7)\n",
    "\n",
    "gpr = VariationalEMSparseGPR(kernel=kernel, white_kernle=white_kernel)\n",
    "ind = gpr.fit(X_train, Y_train, m=80, subsetNum=100, lr=8e-2, episode=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gpr.predict(X_test)\n",
    "print(Loss(pred, Y_test))\n",
    "\n",
    "del gpr, pred, kernel\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gpytorch SVIGPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jylearn.nonparametric.svigpr import IndependentMultitaskGPModel\n",
    "import gpytorch\n",
    "import torch\n",
    "import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1000, 21])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inducing_points = []\n",
    "for i in range(7):\n",
    "    point_set = np.random.choice(np.arange(len(X_train)), 1000, replace=False)\n",
    "    inducing_points.append(X_train[point_set].unsqueeze(0))\n",
    "inducing_points = torch.cat(inducing_points, dim=0).to(device).double()\n",
    "inducing_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IndependentMultitaskGPModel(inducing_points).to(device).double()\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=7).to(device).double()\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=Y_train.size(0), lr=1)\n",
    "\n",
    "hyperparameter_optimizer = torch.optim.Adam([\n",
    "    {'params': model.hyperparameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=Y_train.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 1/50 [04:39<3:47:52, 279.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28469.717058621492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   4%|▍         | 2/50 [09:20<3:44:22, 280.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10869.032150910125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   4%|▍         | 2/50 [09:37<3:50:58, 288.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b88de3707c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mvariational_ngd_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mhyperparameter_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs_iter = tqdm.tqdm(range(50), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    epoch_loss = 0.\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        ### Perform NGD step to optimize variational parameters\n",
    "        variational_ngd_optimizer.zero_grad()\n",
    "        hyperparameter_optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        variational_ngd_optimizer.step()\n",
    "        hyperparameter_optimizer.step()\n",
    "    print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.2292, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test)\n",
    "    means = preds.mean\n",
    "\n",
    "print(Loss(means, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
