{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The benchmark of SARCOS dataset\n",
    "\n",
    "The performance of different algorithms on this dataset can be found in: https://github.com/Kaixhin/SARCOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with polynomial(3) + fourier([0.1:10:20]) feature: 1.1988,  \n",
    "NN feature not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aslearn.data.get_data import robot_inv_data\n",
    "import numpy as np\n",
    "import torch as th\n",
    "device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "\n",
    "Loss = th.nn.MSELoss()\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = robot_inv_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature dim:  2024\n"
     ]
    }
   ],
   "source": [
    "X, Y = np.concatenate([X_train, X_val], axis=0), np.concatenate([Y_train, Y_val], axis=0)\n",
    "from aslearn.feature.global_features import PolynomialFT\n",
    "from aslearn.feature.bellcurve import BellCurve\n",
    "from aslearn.feature.global_features import FourierFT\n",
    "from aslearn.parametric.ridge import RidgeReg\n",
    "\n",
    "f1 = PolynomialFT(degree=3) # sklearn.preprocessing.PolynomialFeature is much faster when len(X) is very large !\n",
    "# f2 = FourierFT(degree=[1.])\n",
    "X = X.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "X_f = f1(X)#np.concatenate([f1(X),f2(X)], axis=1)\n",
    "X_test_f = f1(X_test)#np.concatenate([f1(X_test),f2(X_test)], axis=1)\n",
    "print(\"feature dim: \", X_f.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [02:01<00:00, 12.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2818, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "X_f_t, Y_t, X_test_f_t, Y_t_test = \\\n",
    "th.from_numpy(X_f).to(device).double(), th.from_numpy(Y).to(device).double(),\\\n",
    "th.from_numpy(X_test_f).to(device).double(), th.from_numpy(Y_test).to(device).double()\n",
    "rr = RidgeReg().fit(X_f_t, Y_t)\n",
    "pred = rr.predict(X_test_f_t)\n",
    "mse = Loss(pred, Y_t_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VRVM: 1.3459, maximum likelihood linear reg: 1.2800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interation: 1 Curr_tol:  2765.6447\n",
      "Interation: 2 Curr_tol:  0.0114\n",
      "Interation: 3 Curr_tol:  0.0049\n",
      "Interation: 4 Curr_tol:  0.0082\n",
      "Interation: 5 Curr_tol:  0.0041\n",
      "Interation: 6 Curr_tol:  0.0156\n",
      "Interation: 7 Curr_tol:  0.0045\n",
      "Interation: 8 Curr_tol:  0.0033\n",
      "Interation: 9 Curr_tol:  0.0078\n",
      "Interation: 10 Curr_tol:  0.0118\n",
      "Interation: 11 Curr_tol:  0.0049\n",
      "Interation: 12 Curr_tol:  0.0061\n",
      "Interation: 13 Curr_tol:  0.0070\n",
      "Interation: 14 Curr_tol:  0.0033\n",
      "Interation: 15 Curr_tol:  0.0076\n",
      "Interation: 16 Curr_tol:  0.0104\n",
      "Interation: 17 Curr_tol:  0.0032\n",
      "Interation: 18 Curr_tol:  0.0037\n",
      "Interation: 19 Curr_tol:  0.0082\n",
      "Interation: 20 Curr_tol:  0.0046\n",
      "tensor(1.2800e+00, device='cuda:0', dtype=torch.float64)\n",
      "CPU times: user 8.33 s, sys: 4.39 ms, total: 8.33 s\n",
      "Wall time: 8.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from aslearn.parametric.mllr import MLLR\n",
    "blr = MLLR().fit(X_f_t, Y_t, info_level=1)\n",
    "pred = blr.predict(X_test_f_t)\n",
    "mse = Loss(pred, Y_t_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test =\\\n",
    "    th.from_numpy(X_train).to(device).double(), th.from_numpy(Y_train).to(device).double(), th.from_numpy(X_val).to(device).double(), \\\n",
    "    th.from_numpy(Y_val).to(device).double(), th.from_numpy(X_test).to(device).double(), th.from_numpy(Y_test).to(device).double()\n",
    "    \n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from aslearn.parametric.mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network: 1.469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=21, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=500, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "Curr validation loss:  tensor(23.1734, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(18.8395, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(14.8275, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(13.2116, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(12.2317, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(10.8721, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(9.9622, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(10.9913, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(9.5698, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(9.7632, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.8167, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(9.9699, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.7152, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.6551, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.7575, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.2959, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.8536, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.0767, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.9499, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.1237, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.9751, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.7323, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4384, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.1640, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0358, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.8429, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.6863, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2018, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.6647, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.8625, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3257, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0713, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3034, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0909, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3345, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4222, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4062, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.5613, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1777, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1661, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3619, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4093, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3931, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2502, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8095, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1202, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2148, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0301, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0051, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3572, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1880, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3078, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2701, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1843, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0512, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1647, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0762, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7948, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8211, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.5198, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.9569, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8869, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8773, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2458, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8567, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.5757, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8663, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2095, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1336, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8608, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.6692, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7612, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.9580, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7878, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3272, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3505, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1388, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8889, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4718, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1030, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3298, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8033, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1207, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7425, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7574, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4626, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8514, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.9602, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1574, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7241, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.9709, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.6559, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7324, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0405, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1060, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.9064, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8701, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8803, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7179, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2705, device='cuda:0', dtype=torch.float64)\n",
      "tensor(1.5757, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "param = {\"layer\":4, \"nodes\":[21, 500, 500, 7], \"actfunc\":[\"ReLU\", \"ReLU\", None], \"batch\":128, \"lr\":1e-3, \"decay\":0.}\n",
    "net = MLP(param).to(device).double()\n",
    "print(net)\n",
    "Loss = MSELoss()\n",
    "parameters = MLP.setParams(net, param[\"decay\"])\n",
    "optimizer = Adam(parameters, lr=param[\"lr\"])\n",
    "for _ in range(100):\n",
    "    for i in range(len(X_train)//param[\"batch\"]):\n",
    "        optimizer.zero_grad()\n",
    "        index = th.randperm(len(X_train))\n",
    "        curr_index = index[i*param[\"batch\"]:(i+1)*param[\"batch\"]]\n",
    "        X_b = X_train[curr_index]\n",
    "        Y_b = Y_train[curr_index]\n",
    "        pred = net(X_b)\n",
    "        L = Loss(pred, Y_b)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    with th.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        L_val = Loss(pred_val, Y_val)\n",
    "        print(\"Curr validation loss: \", L_val)\n",
    "net.eval()\n",
    "pred_test = net(X_test)\n",
    "print(Loss(pred_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsample GPR with hyperparameters optimization: 2.887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aslearn.nonparametric.gpr import ExactGPR\n",
    "from aslearn.kernel.kernels import White, RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = X_test.double(), Y_test.double()\n",
    "perm = th.randperm(X_train.size(0))\n",
    "idx = perm[:3000]\n",
    "X_train_, Y_train_ = X_train[idx].double(), Y_train[idx].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aslearn.parametric.mllr import MLLR\n",
    "mp = MLLR().fit(X_train, Y_train) # the linear mean prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = th.ones((21,7)) * 20.\n",
    "c1 = th.ones((7,)) * 0.5\n",
    "c2 = th.ones((7,)) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: inf\n"
     ]
    }
   ],
   "source": [
    "kernel = White(c=c1, dim_in=21, dim_out=7) + RBF(l=l, c=c2, dim_in=21, dim_out=7)\n",
    "gpr = ExactGPR(kernel=kernel).fit(X_train_, Y_train_, mean_prior=None, info_level=1, call_opt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    params = list(gpr.kernel.parameters())\n",
    "    c1 = params[0].data\n",
    "    c2 = params[1].data[:7]\n",
    "    l = params[1].data[7:].reshape(21,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3950e+00, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pred = gpr.predict(X_test)\n",
    "print(Loss(pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titsias' variational EM GPR: m=250, 2.8788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current mean elbo: -718055.41:  85%|██████████▏ | 17/20 [07:03<01:21, 27.27s/it]"
     ]
    }
   ],
   "source": [
    "from aslearn.nonparametric.vigpr import VariationalEMSparseGPR\n",
    "from aslearn.kernel.kernels import White, RBF\n",
    "\n",
    "l = np.ones([21, 1]) * 1.\n",
    "sigma = np.ones([1,]) * 1.\n",
    "c = np.ones([1,]) * 0.1\n",
    "\n",
    "white_kernel = White(c=c, dim_in=21, dim_out=1)\n",
    "kernel = RBF(sigma=sigma, l=l, dim_in=21, dim_out=1)\n",
    "\n",
    "gpr = VariationalEMSparseGPR(kernel=kernel, white_kernle=white_kernel)\n",
    "ind = gpr.fit(X_train, Y_train, m=20, subsetNum=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.2787, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = gpr.predict(X_test)\n",
    "print(Loss(pred, Y_test))\n",
    "\n",
    "del gpr, pred, kernel\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abb",
   "language": "python",
   "name": "abb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
