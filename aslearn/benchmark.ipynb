{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The benchmark of SARCOS dataset\n",
    "\n",
    "The performance of different algorithms on this dataset can be found in: https://github.com/Kaixhin/SARCOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with polynomial(3) + fourier([0.1:10:20]) feature: 1.1988,  \n",
    "NN feature not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aslearn.data.get_data import robot_inv_data\n",
    "import numpy as np\n",
    "import torch as th\n",
    "device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "\n",
    "Loss = th.nn.MSELoss()\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = robot_inv_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature dim:  2024\n"
     ]
    }
   ],
   "source": [
    "X, Y = np.concatenate([X_train, X_val], axis=0), np.concatenate([Y_train, Y_val], axis=0)\n",
    "from aslearn.feature.global_features import PolynomialFT\n",
    "from aslearn.feature.bellcurve import BellCurve\n",
    "from aslearn.feature.global_features import FourierFT\n",
    "from aslearn.parametric.ridge import RidgeReg\n",
    "\n",
    "f1 = PolynomialFT(degree=3) # sklearn.preprocessing.PolynomialFeature is much faster when len(X) is very large !\n",
    "# f2 = FourierFT(degree=[1.])\n",
    "X = X.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "X_f = f1(X)#np.concatenate([f1(X),f2(X)], axis=1)\n",
    "X_test_f = f1(X_test)#np.concatenate([f1(X_test),f2(X_test)], axis=1)\n",
    "print(\"feature dim: \", X_f.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m X_f_t, Y_t, X_test_f_t, Y_t_test \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m      2\u001b[0m th\u001b[38;5;241m.\u001b[39mfrom_numpy(X_f)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mdouble(), th\u001b[38;5;241m.\u001b[39mfrom_numpy(Y)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mdouble(),\\\n\u001b[1;32m      3\u001b[0m th\u001b[38;5;241m.\u001b[39mfrom_numpy(X_test_f)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mdouble(), th\u001b[38;5;241m.\u001b[39mfrom_numpy(Y_test)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[0;32m----> 4\u001b[0m rr \u001b[38;5;241m=\u001b[39m \u001b[43mRidgeReg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_f_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m pred \u001b[38;5;241m=\u001b[39m rr\u001b[38;5;241m.\u001b[39mpredict(X_test_f_t)\n\u001b[1;32m      6\u001b[0m mse \u001b[38;5;241m=\u001b[39m Loss(pred, Y_t_test)\n",
      "File \u001b[0;32m~/Desktop/MY_ML/aslearn/parametric/ridge.py:35\u001b[0m, in \u001b[0;36mRidgeReg.fit\u001b[0;34m(self, X, Y, K, plot_vali_loss)\u001b[0m\n\u001b[1;32m     33\u001b[0m I_diag \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m*\u001b[39m labd\n\u001b[1;32m     34\u001b[0m I_diag[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 35\u001b[0m weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39minverse(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m \u001b[38;5;241m+\u001b[39m I_diag) \u001b[38;5;241m@\u001b[39m X_train\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m Y_train\n\u001b[1;32m     37\u001b[0m pred \u001b[38;5;241m=\u001b[39m X_val \u001b[38;5;241m@\u001b[39m weight\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(pred \u001b[38;5;241m-\u001b[39m Y_val, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y_val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_f_t, Y_t, X_test_f_t, Y_t_test = \\\n",
    "th.from_numpy(X_f).to(device).double(), th.from_numpy(Y).to(device).double(),\\\n",
    "th.from_numpy(X_test_f).to(device).double(), th.from_numpy(Y_test).to(device).double()\n",
    "rr = RidgeReg().fit(X_f_t, Y_t)\n",
    "pred = rr.predict(X_test_f_t)\n",
    "mse = Loss(pred, Y_t_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VRVM: 1.3459, maximum likelihood linear reg: 1.2800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter:  1  Curr_tol:  2765.6472970507034\n",
      "Inter:  2  Curr_tol:  0.009842628613114357\n",
      "Inter:  3  Curr_tol:  0.005301166791468859\n",
      "Inter:  4  Curr_tol:  0.005824516527354717\n",
      "Inter:  5  Curr_tol:  0.004652787931263447\n",
      "Inter:  6  Curr_tol:  0.013454971369355917\n",
      "Inter:  7  Curr_tol:  0.00451575790066272\n",
      "Inter:  8  Curr_tol:  0.005873679183423519\n",
      "Inter:  9  Curr_tol:  0.005632089916616678\n",
      "Inter:  10  Curr_tol:  0.006531859282404184\n",
      "tensor(1.2800, device='cuda:0', dtype=torch.float64)\n",
      "CPU times: user 5.27 s, sys: 0 ns, total: 5.27 s\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from aslearn.parametric.ml_lr import ML_LR\n",
    "blr = ML_LR(nx=2024, ny=7).fit(X_f_t, Y_t, max_iter=10)\n",
    "pred = blr.predict(X_test_f_t)\n",
    "mse = Loss(pred, Y_t_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test =\\\n",
    "    th.from_numpy(X_train).to(device).double(), th.from_numpy(Y_train).to(device).double(), th.from_numpy(X_val).to(device).double(), \\\n",
    "    th.from_numpy(Y_val).to(device).double(), th.from_numpy(X_test).to(device).double(), th.from_numpy(Y_test).to(device).double()\n",
    "    \n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from aslearn.parametric.mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network: 1.469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=21, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=500, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "Curr validation loss:  tensor(23.1734, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(18.8395, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(14.8275, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(13.2116, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(12.2317, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(10.8721, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(9.9622, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(10.9913, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(9.5698, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(9.7632, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.8167, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(9.9699, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.7152, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.6551, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.7575, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.2959, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.8536, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.0767, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.9499, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.1237, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.9751, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.7323, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4384, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(8.1640, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0358, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.8429, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.6863, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2018, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.6647, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.8625, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3257, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0713, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3034, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0909, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3345, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4222, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4062, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.5613, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1777, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1661, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3619, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4093, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3931, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2502, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8095, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1202, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2148, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0301, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0051, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3572, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1880, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3078, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2701, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1843, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0512, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1647, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0762, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7948, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8211, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.5198, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.9569, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8869, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8773, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2458, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8567, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.5757, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8663, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2095, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1336, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8608, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.6692, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7612, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.9580, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7878, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3272, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3505, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1388, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8889, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4718, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1030, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.3298, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8033, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1207, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7425, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7574, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.4626, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8514, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.9602, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1574, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7241, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.9709, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.6559, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7324, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.0405, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.1060, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.9064, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8701, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.8803, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(6.7179, device='cuda:0', dtype=torch.float64)\n",
      "Curr validation loss:  tensor(7.2705, device='cuda:0', dtype=torch.float64)\n",
      "tensor(1.5757, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "param = {\"layer\":4, \"nodes\":[21, 500, 500, 7], \"actfunc\":[\"ReLU\", \"ReLU\", None], \"batch\":128, \"lr\":1e-3, \"decay\":0.}\n",
    "net = MLP(param).to(device).double()\n",
    "print(net)\n",
    "Loss = MSELoss()\n",
    "parameters = MLP.setParams(net, param[\"decay\"])\n",
    "optimizer = Adam(parameters, lr=param[\"lr\"])\n",
    "for _ in range(100):\n",
    "    for i in range(len(X_train)//param[\"batch\"]):\n",
    "        optimizer.zero_grad()\n",
    "        index = th.randperm(len(X_train))\n",
    "        curr_index = index[i*param[\"batch\"]:(i+1)*param[\"batch\"]]\n",
    "        X_b = X_train[curr_index]\n",
    "        Y_b = Y_train[curr_index]\n",
    "        pred = net(X_b)\n",
    "        L = Loss(pred, Y_b)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    with th.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        L_val = Loss(pred_val, Y_val)\n",
    "        print(\"Curr validation loss: \", L_val)\n",
    "net.eval()\n",
    "pred_test = net(X_test)\n",
    "print(Loss(pred_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsample GPR with hyperparameters optimization: 2.887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aslearn.nonparametric.gpr import ExactGPR\n",
    "from aslearn.kernel.kernels import White, Matern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = X_test.double(), Y_test.double()\n",
    "perm = th.randperm(X_train.size(0))\n",
    "idx = perm[:3000]\n",
    "X_train_, Y_train_ = X_train[idx].double(), Y_train[idx].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  100 Current evidence: -12027.72\n",
      "Step:  200 Current evidence: -15308.35\n",
      "Step:  300 Current evidence: -18122.23\n",
      "Step:  400 Current evidence: -17258.64\n",
      "Step:  500 Current evidence: -13866.47\n",
      "Step:  600 Current evidence: -13648.89\n",
      "Step:  700 Current evidence: -13921.32\n",
      "Step:  800 Current evidence: -16706.18\n",
      "Step:  900 Current evidence: -12643.77\n",
      "Step:  1000 Current evidence: -12013.69\n",
      "Step:  1100 Current evidence: -15788.48\n",
      "Step:  1200 Current evidence: -12152.02\n",
      "Step:  1300 Current evidence: -13904.80\n",
      "Step:  1400 Current evidence: -11431.40\n",
      "Step:  1500 Current evidence: -11305.77\n",
      "Step:  1600 Current evidence: -17427.09\n",
      "Step:  1700 Current evidence: -13249.40\n",
      "Step:  1800 Current evidence: -14701.76\n",
      "Step:  1900 Current evidence: -11861.40\n",
      "Step:  2000 Current evidence: -13667.09\n",
      "Step:  2100 Current evidence: -13001.84\n",
      "Step:  2200 Current evidence: -11222.64\n",
      "Step:  2300 Current evidence: -9233.24\n",
      "Step:  2400 Current evidence: -11248.33\n",
      "Step:  2500 Current evidence: -10862.03\n",
      "Step:  2600 Current evidence: -11957.60\n",
      "Step:  2700 Current evidence: -12223.81\n",
      "Step:  2800 Current evidence: -11397.83\n",
      "Step:  2900 Current evidence: -11233.79\n",
      "Step:  3000 Current evidence: -10437.16\n",
      "Step:  3100 Current evidence: -12454.83\n",
      "Step:  3200 Current evidence: -10794.91\n",
      "Step:  3300 Current evidence: -10371.30\n",
      "Step:  3400 Current evidence: -10268.09\n",
      "Step:  3500 Current evidence: -9991.90\n",
      "Step:  3600 Current evidence: -10308.97\n",
      "Step:  3700 Current evidence: -10932.27\n",
      "Step:  3800 Current evidence: -10215.38\n",
      "Step:  3900 Current evidence: -10982.24\n",
      "Step:  4000 Current evidence: -7290.44\n",
      "Step:  4100 Current evidence: -10485.19\n",
      "Step:  4200 Current evidence: -8277.36\n",
      "Step:  4300 Current evidence: -7616.47\n",
      "Step:  4400 Current evidence: -7323.19\n",
      "Step:  4500 Current evidence: -7610.21\n",
      "Step:  4600 Current evidence: -9962.72\n",
      "Step:  4700 Current evidence: -7977.99\n",
      "Step:  4800 Current evidence: -9359.38\n",
      "Step:  4900 Current evidence: -8052.56\n",
      "Step:  5000 Current evidence: -7896.99\n",
      "Step:  5100 Current evidence: -7876.08\n",
      "Step:  5200 Current evidence: -8546.99\n",
      "Step:  5300 Current evidence: -7101.19\n",
      "Step:  5400 Current evidence: -6905.12\n",
      "Step:  5500 Current evidence: -8569.23\n",
      "Step:  5600 Current evidence: -6846.61\n",
      "Step:  5700 Current evidence: -7408.43\n",
      "Step:  5800 Current evidence: -6442.35\n",
      "Step:  5900 Current evidence: -6186.39\n",
      "Step:  6000 Current evidence: -6832.24\n",
      "Step:  6100 Current evidence: -5205.03\n",
      "Step:  6200 Current evidence: -5805.71\n",
      "Step:  6300 Current evidence: -5780.28\n",
      "Step:  6400 Current evidence: -6030.55\n",
      "Step:  6500 Current evidence: -5604.23\n",
      "Step:  6600 Current evidence: -5860.11\n",
      "Step:  6700 Current evidence: -4713.48\n",
      "Step:  6800 Current evidence: -6122.91\n",
      "Step:  6900 Current evidence: -4512.97\n",
      "Step:  7000 Current evidence: -5708.11\n",
      "Step:  7100 Current evidence: -4965.70\n",
      "Step:  7200 Current evidence: -3739.96\n",
      "Step:  7300 Current evidence: -4900.61\n",
      "Step:  7400 Current evidence: -4254.04\n",
      "Step:  7500 Current evidence: -4199.53\n",
      "Step:  7600 Current evidence: -4130.00\n",
      "Step:  7700 Current evidence: -4644.29\n",
      "Step:  7800 Current evidence: -4925.99\n",
      "Step:  7900 Current evidence: -5198.66\n",
      "Step:  8000 Current evidence: -3625.22\n",
      "Step:  8100 Current evidence: -3741.43\n",
      "Step:  8200 Current evidence: -3787.62\n",
      "Step:  8300 Current evidence: -4365.00\n",
      "Step:  8400 Current evidence: -3395.89\n",
      "Step:  8500 Current evidence: -3301.29\n",
      "Step:  8600 Current evidence: -3128.66\n",
      "Step:  8700 Current evidence: -3817.62\n",
      "Step:  8800 Current evidence: -3174.03\n",
      "Step:  8900 Current evidence: -3320.67\n",
      "Step:  9000 Current evidence: -2629.84\n",
      "Step:  9100 Current evidence: -3772.88\n",
      "Step:  9200 Current evidence: -3586.90\n",
      "Step:  9300 Current evidence: -3558.05\n",
      "Step:  9400 Current evidence: -3600.04\n",
      "Step:  9500 Current evidence: -2786.41\n",
      "Step:  9600 Current evidence: -2564.08\n",
      "Step:  9700 Current evidence: -2239.19\n",
      "Step:  9800 Current evidence: -2227.29\n",
      "Step:  9900 Current evidence: -2614.15\n",
      "Step:  10000 Current evidence: -3790.01\n",
      "Step:  10100 Current evidence: -1967.40\n",
      "Step:  10200 Current evidence: -2692.61\n",
      "Step:  10300 Current evidence: -2267.10\n",
      "Step:  10400 Current evidence: -2114.28\n",
      "Step:  10500 Current evidence: -1694.10\n",
      "Step:  10600 Current evidence: -2420.73\n",
      "Step:  10700 Current evidence: -2008.49\n",
      "Step:  10800 Current evidence: -1799.01\n",
      "Step:  10900 Current evidence: -1545.90\n",
      "Step:  11000 Current evidence: -1489.16\n",
      "Step:  11100 Current evidence: -1239.22\n",
      "Step:  11200 Current evidence: -2016.99\n",
      "Step:  11300 Current evidence: -1508.60\n",
      "Step:  11400 Current evidence: -1752.25\n",
      "Step:  11500 Current evidence: -1238.25\n",
      "Step:  11600 Current evidence: -1156.30\n",
      "Step:  11700 Current evidence: -971.50\n",
      "Step:  11800 Current evidence: -1145.71\n",
      "Step:  11900 Current evidence: -909.79\n",
      "Step:  12000 Current evidence: -1119.41\n",
      "Step:  12100 Current evidence: -850.38\n",
      "Step:  12200 Current evidence: -843.38\n",
      "Step:  12300 Current evidence: -818.85\n",
      "Step:  12400 Current evidence: -757.37\n",
      "Step:  12500 Current evidence: -869.45\n",
      "Step:  12600 Current evidence: -716.82\n",
      "Step:  12700 Current evidence: -498.70\n",
      "Step:  12800 Current evidence: -550.38\n",
      "Step:  12900 Current evidence: -405.71\n",
      "Step:  13000 Current evidence: -573.47\n",
      "Step:  13100 Current evidence: -480.05\n",
      "Step:  13200 Current evidence: -458.42\n",
      "Step:  13300 Current evidence: -413.67\n",
      "Step:  13400 Current evidence: -470.84\n",
      "Step:  13500 Current evidence: -564.66\n",
      "Step:  13600 Current evidence: -452.45\n",
      "Step:  13700 Current evidence: -432.97\n",
      "Step:  13800 Current evidence: -497.10\n",
      "Step:  13900 Current evidence: -430.80\n",
      "Step:  14000 Current evidence: -504.98\n",
      "Step:  14100 Current evidence: -398.42\n",
      "Step:  14200 Current evidence: -421.57\n",
      "Step:  14300 Current evidence: -462.79\n",
      "Step:  14400 Current evidence: -436.27\n",
      "Step:  14500 Current evidence: -380.91\n",
      "Step:  14600 Current evidence: -527.32\n",
      "Step:  14700 Current evidence: -446.81\n",
      "Step:  14800 Current evidence: -428.01\n",
      "Step:  14900 Current evidence: -444.81\n",
      "Step:  15000 Current evidence: -377.05\n",
      "Step:  15100 Current evidence: -444.38\n",
      "Step:  15200 Current evidence: -383.35\n",
      "Step:  15300 Current evidence: -505.59\n",
      "Step:  15400 Current evidence: -407.00\n",
      "Step:  15500 Current evidence: -357.32\n",
      "Step:  15600 Current evidence: -375.74\n",
      "Step:  15700 Current evidence: -476.24\n",
      "Step:  15800 Current evidence: -352.29\n",
      "Step:  15900 Current evidence: -394.87\n",
      "Step:  16000 Current evidence: -374.04\n",
      "Step:  16100 Current evidence: -337.16\n",
      "Step:  16200 Current evidence: -330.05\n",
      "Step:  16300 Current evidence: -299.97\n",
      "Step:  16400 Current evidence: -289.97\n",
      "Step:  16500 Current evidence: -320.86\n",
      "Step:  16600 Current evidence: -311.74\n",
      "Step:  16700 Current evidence: -314.25\n",
      "Step:  16800 Current evidence: -321.03\n",
      "Step:  16900 Current evidence: -436.53\n",
      "Step:  17000 Current evidence: -376.34\n",
      "Step:  17100 Current evidence: -331.85\n",
      "Step:  17200 Current evidence: -326.77\n",
      "Step:  17300 Current evidence: -351.15\n",
      "Step:  17400 Current evidence: -316.07\n",
      "Step:  17500 Current evidence: -299.22\n",
      "Step:  17600 Current evidence: -329.51\n",
      "Step:  17700 Current evidence: -330.92\n",
      "Step:  17800 Current evidence: -323.15\n",
      "Step:  17900 Current evidence: -268.07\n",
      "Step:  18000 Current evidence: -308.63\n",
      "Step:  18100 Current evidence: -351.87\n",
      "Step:  18200 Current evidence: -260.69\n",
      "Step:  18300 Current evidence: -278.97\n",
      "Step:  18400 Current evidence: -326.99\n",
      "Step:  18500 Current evidence: -262.77\n",
      "Step:  18600 Current evidence: -267.06\n",
      "Step:  18700 Current evidence: -264.69\n",
      "Step:  18800 Current evidence: -280.71\n",
      "Step:  18900 Current evidence: -302.94\n",
      "Step:  19000 Current evidence: -313.08\n",
      "Step:  19100 Current evidence: -315.19\n",
      "Step:  19200 Current evidence: -330.27\n",
      "Step:  19300 Current evidence: -268.21\n",
      "Step:  19400 Current evidence: -293.52\n",
      "Step:  19500 Current evidence: -237.79\n",
      "Step:  19600 Current evidence: -310.12\n",
      "Step:  19700 Current evidence: -260.92\n",
      "Step:  19800 Current evidence: -276.40\n",
      "Step:  19900 Current evidence: -248.15\n",
      "Step:  20000 Current evidence: -269.58\n",
      "Step:  20100 Current evidence: -232.83\n",
      "Step:  20200 Current evidence: -236.11\n",
      "Step:  20300 Current evidence: -246.22\n",
      "Step:  20400 Current evidence: -338.77\n",
      "Step:  20500 Current evidence: -238.27\n",
      "Step:  20600 Current evidence: -262.80\n",
      "Step:  20700 Current evidence: -253.81\n",
      "Step:  20800 Current evidence: -251.66\n",
      "Step:  20900 Current evidence: -243.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  21000 Current evidence: -242.22\n",
      "Step:  21100 Current evidence: -226.98\n",
      "Step:  21200 Current evidence: -320.51\n",
      "Step:  21300 Current evidence: -236.90\n",
      "Step:  21400 Current evidence: -252.03\n",
      "Step:  21500 Current evidence: -235.42\n",
      "Step:  21600 Current evidence: -277.05\n",
      "Step:  21700 Current evidence: -236.82\n",
      "Step:  21800 Current evidence: -221.21\n",
      "Step:  21900 Current evidence: -247.21\n",
      "Step:  22000 Current evidence: -271.40\n",
      "Step:  22100 Current evidence: -273.12\n",
      "Step:  22200 Current evidence: -225.60\n",
      "Step:  22300 Current evidence: -213.60\n",
      "Step:  22400 Current evidence: -222.83\n",
      "Step:  22500 Current evidence: -247.51\n",
      "Step:  22600 Current evidence: -211.82\n",
      "Step:  22700 Current evidence: -213.51\n",
      "Step:  22800 Current evidence: -217.26\n",
      "Step:  22900 Current evidence: -214.83\n",
      "Step:  23000 Current evidence: -212.92\n",
      "Step:  23100 Current evidence: -211.42\n",
      "Step:  23200 Current evidence: -215.14\n",
      "Step:  23300 Current evidence: -205.82\n",
      "Step:  23400 Current evidence: -230.45\n",
      "Step:  23500 Current evidence: -199.81\n",
      "Step:  23600 Current evidence: -227.18\n",
      "Step:  23700 Current evidence: -212.02\n",
      "Step:  23800 Current evidence: -254.41\n",
      "Step:  23900 Current evidence: -223.48\n",
      "Step:  24000 Current evidence: -181.05\n",
      "Step:  24100 Current evidence: -214.66\n",
      "Step:  24200 Current evidence: -221.55\n",
      "Step:  24300 Current evidence: -204.71\n",
      "Step:  24400 Current evidence: -212.24\n",
      "Step:  24500 Current evidence: -206.04\n",
      "Step:  24600 Current evidence: -220.41\n",
      "Step:  24700 Current evidence: -188.19\n",
      "Step:  24800 Current evidence: -199.50\n",
      "Step:  24900 Current evidence: -225.66\n",
      "Step:  25000 Current evidence: -211.70\n",
      "Step:  25100 Current evidence: -208.90\n",
      "Step:  25200 Current evidence: -194.74\n",
      "Step:  25300 Current evidence: -206.92\n",
      "Step:  25400 Current evidence: -200.87\n",
      "Step:  25500 Current evidence: -186.83\n",
      "Step:  25600 Current evidence: -207.28\n",
      "Step:  25700 Current evidence: -228.04\n",
      "Step:  25800 Current evidence: -209.12\n",
      "Step:  25900 Current evidence: -210.37\n",
      "Step:  26000 Current evidence: -220.16\n",
      "Step:  26100 Current evidence: -200.00\n",
      "Step:  26200 Current evidence: -190.73\n",
      "Step:  26300 Current evidence: -196.73\n",
      "Step:  26400 Current evidence: -192.41\n",
      "Step:  26500 Current evidence: -175.37\n",
      "Step:  26600 Current evidence: -183.50\n",
      "Step:  26700 Current evidence: -188.50\n",
      "Step:  26800 Current evidence: -184.05\n",
      "Step:  26900 Current evidence: -207.05\n",
      "Step:  27000 Current evidence: -201.32\n",
      "Step:  27100 Current evidence: -186.53\n",
      "Step:  27200 Current evidence: -185.09\n",
      "Step:  27300 Current evidence: -185.31\n",
      "Step:  27400 Current evidence: -185.51\n",
      "Step:  27500 Current evidence: -195.37\n",
      "Step:  27600 Current evidence: -196.76\n",
      "Step:  27700 Current evidence: -195.34\n",
      "Step:  27800 Current evidence: -188.11\n",
      "Step:  27900 Current evidence: -176.98\n",
      "Step:  28000 Current evidence: -168.57\n",
      "Step:  28100 Current evidence: -185.94\n",
      "Step:  28200 Current evidence: -182.84\n",
      "Step:  28300 Current evidence: -181.53\n",
      "Step:  28400 Current evidence: -172.40\n",
      "Step:  28500 Current evidence: -184.82\n",
      "Step:  28600 Current evidence: -161.76\n",
      "Step:  28700 Current evidence: -173.47\n",
      "Step:  28800 Current evidence: -180.89\n",
      "Step:  28900 Current evidence: -179.63\n",
      "Step:  29000 Current evidence: -192.06\n",
      "Step:  29100 Current evidence: -189.89\n",
      "Step:  29200 Current evidence: -171.23\n",
      "Step:  29300 Current evidence: -183.85\n",
      "Step:  29400 Current evidence: -167.43\n",
      "Step:  29500 Current evidence: -162.01\n",
      "Step:  29600 Current evidence: -165.16\n",
      "Step:  29700 Current evidence: -176.97\n",
      "Step:  29800 Current evidence: -192.32\n",
      "Step:  29900 Current evidence: -172.27\n",
      "Step:  30000 Current evidence: -163.37\n",
      "Step:  30100 Current evidence: -180.24\n",
      "Step:  30200 Current evidence: -174.03\n",
      "Step:  30300 Current evidence: -167.49\n",
      "Step:  30400 Current evidence: -185.13\n",
      "Step:  30500 Current evidence: -178.54\n",
      "Step:  30600 Current evidence: -168.13\n",
      "Step:  30700 Current evidence: -181.18\n",
      "Step:  30800 Current evidence: -176.45\n",
      "Step:  30900 Current evidence: -188.82\n",
      "Step:  31000 Current evidence: -165.89\n",
      "Step:  31100 Current evidence: -159.69\n",
      "Step:  31200 Current evidence: -176.26\n",
      "Step:  31300 Current evidence: -167.92\n",
      "Step:  31400 Current evidence: -171.79\n",
      "Step:  31500 Current evidence: -177.80\n",
      "Step:  31600 Current evidence: -170.20\n",
      "Step:  31700 Current evidence: -160.47\n",
      "Step:  31800 Current evidence: -178.12\n",
      "Step:  31900 Current evidence: -163.74\n",
      "Step:  32000 Current evidence: -178.96\n",
      "Step:  32100 Current evidence: -158.38\n",
      "Step:  32200 Current evidence: -170.10\n",
      "Step:  32300 Current evidence: -162.92\n",
      "Step:  32400 Current evidence: -151.46\n",
      "Step:  32500 Current evidence: -165.79\n",
      "Step:  32600 Current evidence: -174.74\n",
      "Step:  32700 Current evidence: -168.28\n",
      "Step:  32800 Current evidence: -166.05\n",
      "Step:  32900 Current evidence: -158.47\n",
      "Step:  33000 Current evidence: -148.76\n",
      "Step:  33100 Current evidence: -164.47\n",
      "Step:  33200 Current evidence: -152.10\n",
      "Step:  33300 Current evidence: -168.04\n",
      "Step:  33400 Current evidence: -156.28\n",
      "Step:  33500 Current evidence: -168.77\n",
      "Step:  33600 Current evidence: -155.86\n",
      "Step:  33700 Current evidence: -167.69\n",
      "Step:  33800 Current evidence: -156.38\n",
      "Step:  33900 Current evidence: -158.69\n",
      "Step:  34000 Current evidence: -171.02\n",
      "Step:  34100 Current evidence: -144.79\n",
      "Step:  34200 Current evidence: -158.55\n",
      "Step:  34300 Current evidence: -154.19\n",
      "Step:  34400 Current evidence: -148.38\n",
      "Step:  34500 Current evidence: -148.37\n",
      "Step:  34600 Current evidence: -171.16\n",
      "Step:  34700 Current evidence: -156.43\n",
      "Step:  34800 Current evidence: -157.59\n",
      "Step:  34900 Current evidence: -160.30\n",
      "Step:  35000 Current evidence: -164.36\n",
      "Step:  35100 Current evidence: -154.37\n",
      "Step:  35200 Current evidence: -165.80\n",
      "Step:  35300 Current evidence: -174.89\n",
      "Step:  35400 Current evidence: -158.66\n",
      "Step:  35500 Current evidence: -156.04\n",
      "Step:  35600 Current evidence: -164.17\n",
      "Step:  35700 Current evidence: -168.01\n",
      "Step:  35800 Current evidence: -152.59\n",
      "Step:  35900 Current evidence: -161.84\n",
      "Step:  36000 Current evidence: -165.97\n",
      "Step:  36100 Current evidence: -155.83\n",
      "Step:  36200 Current evidence: -160.30\n",
      "Step:  36300 Current evidence: -161.86\n",
      "Step:  36400 Current evidence: -157.65\n",
      "Step:  36500 Current evidence: -149.63\n",
      "Step:  36600 Current evidence: -154.97\n",
      "Step:  36700 Current evidence: -148.01\n",
      "Step:  36800 Current evidence: -154.90\n",
      "Step:  36900 Current evidence: -158.45\n",
      "Step:  37000 Current evidence: -153.29\n",
      "Step:  37100 Current evidence: -151.75\n",
      "Step:  37200 Current evidence: -155.38\n",
      "Step:  37300 Current evidence: -147.15\n",
      "Step:  37400 Current evidence: -158.87\n",
      "Step:  37500 Current evidence: -142.79\n",
      "Step:  37600 Current evidence: -146.60\n",
      "Step:  37700 Current evidence: -168.19\n",
      "Step:  37800 Current evidence: -157.63\n",
      "Step:  37900 Current evidence: -158.24\n",
      "Step:  38000 Current evidence: -158.98\n",
      "Step:  38100 Current evidence: -149.40\n",
      "Step:  38200 Current evidence: -151.11\n",
      "Step:  38300 Current evidence: -150.75\n",
      "Step:  38400 Current evidence: -145.36\n",
      "Step:  38500 Current evidence: -150.80\n",
      "Step:  38600 Current evidence: -150.17\n",
      "Step:  38700 Current evidence: -158.37\n",
      "Step:  38800 Current evidence: -152.74\n",
      "Step:  38900 Current evidence: -155.92\n",
      "Step:  39000 Current evidence: -150.99\n",
      "Step:  39100 Current evidence: -158.67\n",
      "Step:  39200 Current evidence: -162.11\n",
      "Step:  39300 Current evidence: -168.54\n",
      "Step:  39400 Current evidence: -139.64\n",
      "Step:  39500 Current evidence: -153.31\n",
      "Step:  39600 Current evidence: -155.76\n",
      "Step:  39700 Current evidence: -145.55\n",
      "Step:  39800 Current evidence: -144.70\n",
      "Step:  39900 Current evidence: -145.43\n",
      "Step:  40000 Current evidence: -144.34\n",
      "Step:  40100 Current evidence: -144.55\n",
      "Step:  40200 Current evidence: -143.71\n",
      "Step:  40300 Current evidence: -144.98\n",
      "Step:  40400 Current evidence: -148.19\n",
      "Step:  40500 Current evidence: -149.38\n",
      "Step:  40600 Current evidence: -141.34\n",
      "Step:  40700 Current evidence: -141.06\n",
      "Step:  40800 Current evidence: -136.38\n",
      "Step:  40900 Current evidence: -141.08\n",
      "Step:  41000 Current evidence: -141.39\n",
      "Step:  41100 Current evidence: -146.60\n",
      "Step:  41200 Current evidence: -141.00\n",
      "Step:  41300 Current evidence: -138.80\n",
      "Step:  41400 Current evidence: -147.20\n",
      "Step:  41500 Current evidence: -139.69\n",
      "Step:  41600 Current evidence: -139.28\n",
      "Step:  41700 Current evidence: -144.41\n",
      "Step:  41800 Current evidence: -145.43\n",
      "Step:  41900 Current evidence: -146.83\n",
      "Step:  42000 Current evidence: -137.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  42100 Current evidence: -135.47\n",
      "Step:  42200 Current evidence: -146.55\n",
      "Step:  42300 Current evidence: -137.23\n",
      "Step:  42400 Current evidence: -130.73\n",
      "Step:  42500 Current evidence: -138.05\n",
      "Step:  42600 Current evidence: -147.82\n",
      "Step:  42700 Current evidence: -139.56\n",
      "Step:  42800 Current evidence: -140.85\n",
      "Step:  42900 Current evidence: -139.43\n",
      "Step:  43000 Current evidence: -144.44\n",
      "Step:  43100 Current evidence: -131.62\n",
      "Step:  43200 Current evidence: -135.04\n",
      "Step:  43300 Current evidence: -136.42\n",
      "Step:  43400 Current evidence: -131.66\n",
      "Step:  43500 Current evidence: -141.53\n",
      "Step:  43600 Current evidence: -141.98\n",
      "Step:  43700 Current evidence: -135.77\n",
      "Step:  43800 Current evidence: -137.13\n",
      "Step:  43900 Current evidence: -140.23\n",
      "Step:  44000 Current evidence: -145.08\n",
      "Step:  44100 Current evidence: -138.39\n",
      "Step:  44200 Current evidence: -143.46\n",
      "Step:  44300 Current evidence: -148.35\n",
      "Step:  44400 Current evidence: -129.62\n",
      "Step:  44500 Current evidence: -132.78\n",
      "Step:  44600 Current evidence: -136.16\n",
      "Step:  44700 Current evidence: -140.06\n",
      "Step:  44800 Current evidence: -138.72\n",
      "Step:  44900 Current evidence: -145.28\n",
      "Step:  45000 Current evidence: -136.27\n",
      "Step:  45100 Current evidence: -132.92\n",
      "Step:  45200 Current evidence: -134.34\n",
      "Step:  45300 Current evidence: -143.36\n",
      "Step:  45400 Current evidence: -130.81\n",
      "Step:  45500 Current evidence: -144.77\n",
      "Step:  45600 Current evidence: -137.77\n",
      "Step:  45700 Current evidence: -136.17\n",
      "Step:  45800 Current evidence: -146.64\n",
      "Step:  45900 Current evidence: -129.89\n",
      "Step:  46000 Current evidence: -134.68\n",
      "Step:  46100 Current evidence: -141.81\n",
      "Step:  46200 Current evidence: -138.78\n",
      "Step:  46300 Current evidence: -133.83\n",
      "Step:  46400 Current evidence: -141.42\n",
      "Step:  46500 Current evidence: -136.97\n",
      "Step:  46600 Current evidence: -145.63\n",
      "Step:  46700 Current evidence: -133.88\n",
      "Step:  46800 Current evidence: -137.62\n",
      "Step:  46900 Current evidence: -134.63\n",
      "Step:  47000 Current evidence: -132.17\n",
      "Step:  47100 Current evidence: -130.65\n",
      "Step:  47200 Current evidence: -132.29\n",
      "Step:  47300 Current evidence: -141.04\n",
      "Step:  47400 Current evidence: -132.86\n",
      "Step:  47500 Current evidence: -126.58\n",
      "Step:  47600 Current evidence: -135.84\n",
      "Step:  47700 Current evidence: -131.48\n",
      "Step:  47800 Current evidence: -135.06\n",
      "Step:  47900 Current evidence: -140.00\n",
      "Step:  48000 Current evidence: -131.60\n",
      "Step:  48100 Current evidence: -135.21\n",
      "Step:  48200 Current evidence: -132.06\n",
      "Step:  48300 Current evidence: -142.24\n",
      "Step:  48400 Current evidence: -131.68\n",
      "Step:  48500 Current evidence: -136.73\n",
      "Step:  48600 Current evidence: -133.62\n",
      "Step:  48700 Current evidence: -127.53\n",
      "Step:  48800 Current evidence: -142.97\n",
      "Step:  48900 Current evidence: -139.27\n",
      "Step:  49000 Current evidence: -133.75\n",
      "Step:  49100 Current evidence: -131.37\n",
      "Step:  49200 Current evidence: -136.67\n",
      "Step:  49300 Current evidence: -127.13\n",
      "Step:  49400 Current evidence: -130.82\n",
      "Step:  49500 Current evidence: -127.09\n",
      "Step:  49600 Current evidence: -136.39\n",
      "Step:  49700 Current evidence: -134.79\n",
      "Step:  49800 Current evidence: -138.77\n",
      "Step:  49900 Current evidence: -136.26\n",
      "Step:  50000 Current evidence: -136.07\n",
      "Step:  50100 Current evidence: -131.02\n",
      "Step:  50200 Current evidence: -139.08\n",
      "Step:  50300 Current evidence: -134.96\n",
      "Step:  50400 Current evidence: -139.47\n",
      "Step:  50500 Current evidence: -132.89\n",
      "Step:  50600 Current evidence: -136.39\n",
      "Step:  50700 Current evidence: -133.75\n",
      "Step:  50800 Current evidence: -146.46\n",
      "Step:  50900 Current evidence: -136.87\n",
      "Step:  51000 Current evidence: -144.40\n",
      "Step:  51100 Current evidence: -135.13\n",
      "Step:  51200 Current evidence: -130.40\n",
      "Step:  51300 Current evidence: -130.41\n",
      "Step:  51400 Current evidence: -143.89\n",
      "Step:  51500 Current evidence: -136.28\n",
      "Step:  51600 Current evidence: -140.05\n",
      "Step:  51700 Current evidence: -134.46\n",
      "Step:  51800 Current evidence: -145.12\n",
      "Step:  51900 Current evidence: -130.21\n",
      "Step:  52000 Current evidence: -133.13\n",
      "Step:  52100 Current evidence: -135.37\n",
      "Step:  52200 Current evidence: -132.99\n",
      "Step:  52300 Current evidence: -140.07\n",
      "Step:  52400 Current evidence: -126.03\n",
      "Step:  52500 Current evidence: -139.73\n",
      "Step:  52600 Current evidence: -132.22\n",
      "Step:  52700 Current evidence: -135.91\n",
      "Step:  52800 Current evidence: -130.52\n",
      "Step:  52900 Current evidence: -136.60\n",
      "Step:  53000 Current evidence: -137.28\n",
      "Step:  53100 Current evidence: -137.20\n",
      "Step:  53200 Current evidence: -146.70\n",
      "Step:  53300 Current evidence: -129.47\n",
      "Step:  53400 Current evidence: -140.34\n",
      "Step:  53500 Current evidence: -132.27\n",
      "Step:  53600 Current evidence: -136.06\n",
      "Step:  53700 Current evidence: -134.21\n",
      "Step:  53800 Current evidence: -131.22\n",
      "Step:  53900 Current evidence: -129.80\n",
      "Step:  54000 Current evidence: -128.75\n",
      "Step:  54100 Current evidence: -139.33\n",
      "Step:  54200 Current evidence: -135.36\n",
      "Step:  54300 Current evidence: -135.79\n",
      "Step:  54400 Current evidence: -132.02\n",
      "Step:  54500 Current evidence: -130.58\n",
      "Step:  54600 Current evidence: -134.79\n",
      "Step:  54700 Current evidence: -139.21\n",
      "Step:  54800 Current evidence: -129.22\n",
      "Step:  54900 Current evidence: -124.20\n",
      "Step:  55000 Current evidence: -126.49\n",
      "Step:  55100 Current evidence: -136.50\n",
      "Step:  55200 Current evidence: -129.70\n",
      "Step:  55300 Current evidence: -130.37\n",
      "Step:  55400 Current evidence: -129.29\n",
      "Step:  55500 Current evidence: -142.74\n",
      "Step:  55600 Current evidence: -147.43\n",
      "Step:  55700 Current evidence: -141.20\n",
      "Step:  55800 Current evidence: -131.78\n",
      "Step:  55900 Current evidence: -130.07\n",
      "Step:  56000 Current evidence: -133.92\n",
      "Step:  56100 Current evidence: -131.50\n",
      "Step:  56200 Current evidence: -137.33\n",
      "Step:  56300 Current evidence: -138.18\n",
      "Step:  56400 Current evidence: -132.44\n",
      "Step:  56500 Current evidence: -132.00\n",
      "Step:  56600 Current evidence: -126.15\n",
      "Step:  56700 Current evidence: -130.50\n",
      "Step:  56800 Current evidence: -145.01\n",
      "Step:  56900 Current evidence: -139.00\n",
      "Step:  57000 Current evidence: -142.64\n",
      "Step:  57100 Current evidence: -134.37\n",
      "Step:  57200 Current evidence: -135.62\n",
      "Step:  57300 Current evidence: -132.52\n",
      "Step:  57400 Current evidence: -136.77\n",
      "Step:  57500 Current evidence: -133.03\n",
      "Step:  57600 Current evidence: -133.86\n",
      "Step:  57700 Current evidence: -133.51\n",
      "Step:  57800 Current evidence: -133.99\n",
      "Step:  57900 Current evidence: -134.38\n",
      "Step:  58000 Current evidence: -137.21\n",
      "Step:  58100 Current evidence: -132.10\n",
      "Step:  58200 Current evidence: -133.22\n",
      "Step:  58300 Current evidence: -132.69\n",
      "Step:  58400 Current evidence: -126.34\n",
      "Step:  58500 Current evidence: -139.48\n",
      "Step:  58600 Current evidence: -125.04\n",
      "Step:  58700 Current evidence: -137.65\n",
      "Step:  58800 Current evidence: -135.53\n",
      "Step:  58900 Current evidence: -133.04\n",
      "Step:  59000 Current evidence: -141.78\n",
      "Step:  59100 Current evidence: -132.58\n",
      "Step:  59200 Current evidence: -133.78\n",
      "Step:  59300 Current evidence: -135.14\n",
      "Step:  59400 Current evidence: -130.05\n",
      "Step:  59500 Current evidence: -139.99\n",
      "Step:  59600 Current evidence: -126.81\n",
      "Step:  59700 Current evidence: -131.44\n",
      "Step:  59800 Current evidence: -135.03\n",
      "Step:  59900 Current evidence: -129.42\n",
      "Step:  60000 Current evidence: -135.23\n",
      "Step:  60100 Current evidence: -137.42\n",
      "Step:  60200 Current evidence: -132.84\n",
      "Step:  60300 Current evidence: -135.77\n",
      "Step:  60400 Current evidence: -128.36\n",
      "Step:  60500 Current evidence: -137.39\n",
      "Step:  60600 Current evidence: -136.81\n",
      "Step:  60700 Current evidence: -136.35\n",
      "Step:  60800 Current evidence: -132.37\n",
      "Step:  60900 Current evidence: -141.97\n",
      "Step:  61000 Current evidence: -132.16\n",
      "Step:  61100 Current evidence: -130.08\n",
      "Step:  61200 Current evidence: -128.53\n",
      "Step:  61300 Current evidence: -127.27\n",
      "Step:  61400 Current evidence: -130.57\n",
      "Step:  61500 Current evidence: -134.48\n",
      "Step:  61600 Current evidence: -138.50\n",
      "Step:  61700 Current evidence: -135.01\n",
      "Step:  61800 Current evidence: -138.46\n",
      "Step:  61900 Current evidence: -143.19\n",
      "Step:  62000 Current evidence: -130.67\n",
      "Step:  62100 Current evidence: -131.59\n",
      "Step:  62200 Current evidence: -135.46\n",
      "Step:  62300 Current evidence: -126.06\n",
      "Step:  62400 Current evidence: -127.85\n",
      "Step:  62500 Current evidence: -138.77\n",
      "Step:  62600 Current evidence: -137.96\n",
      "Step:  62700 Current evidence: -140.95\n",
      "Step:  62800 Current evidence: -140.12\n",
      "Step:  62900 Current evidence: -126.14\n",
      "Step:  63000 Current evidence: -131.24\n",
      "Step:  63100 Current evidence: -135.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  63200 Current evidence: -132.48\n",
      "Step:  63300 Current evidence: -137.75\n",
      "Step:  63400 Current evidence: -138.77\n",
      "Step:  63500 Current evidence: -135.49\n",
      "Step:  63600 Current evidence: -130.04\n",
      "Step:  63700 Current evidence: -136.85\n",
      "Step:  63800 Current evidence: -139.62\n",
      "Step:  63900 Current evidence: -132.93\n",
      "Step:  64000 Current evidence: -131.86\n",
      "Step:  64100 Current evidence: -132.74\n",
      "Step:  64200 Current evidence: -141.30\n",
      "Step:  64300 Current evidence: -137.02\n",
      "Step:  64400 Current evidence: -134.35\n",
      "Step:  64500 Current evidence: -130.38\n",
      "Step:  64600 Current evidence: -128.98\n",
      "Step:  64700 Current evidence: -134.28\n",
      "Step:  64800 Current evidence: -138.62\n",
      "Step:  64900 Current evidence: -137.00\n",
      "Step:  65000 Current evidence: -131.23\n",
      "Step:  65100 Current evidence: -131.36\n",
      "Step:  65200 Current evidence: -140.48\n",
      "Step:  65300 Current evidence: -126.54\n",
      "Step:  65400 Current evidence: -130.76\n",
      "Step:  65500 Current evidence: -134.11\n",
      "Step:  65600 Current evidence: -131.40\n",
      "Step:  65700 Current evidence: -126.27\n",
      "Step:  65800 Current evidence: -138.20\n",
      "Step:  65900 Current evidence: -127.89\n",
      "Step:  66000 Current evidence: -132.53\n",
      "Step:  66100 Current evidence: -136.67\n",
      "Step:  66200 Current evidence: -133.12\n",
      "Step:  66300 Current evidence: -132.01\n",
      "Step:  66400 Current evidence: -133.86\n",
      "Step:  66500 Current evidence: -132.74\n",
      "Step:  66600 Current evidence: -126.86\n",
      "Step:  66700 Current evidence: -138.46\n",
      "Step:  66800 Current evidence: -136.21\n",
      "Step:  66900 Current evidence: -135.41\n",
      "Step:  67000 Current evidence: -132.22\n",
      "Step:  67100 Current evidence: -136.32\n",
      "Step:  67200 Current evidence: -130.02\n",
      "Step:  67300 Current evidence: -137.81\n",
      "Step:  67400 Current evidence: -143.20\n",
      "Step:  67500 Current evidence: -143.56\n",
      "Step:  67600 Current evidence: -146.96\n",
      "Step:  67700 Current evidence: -127.16\n",
      "Step:  67800 Current evidence: -129.39\n",
      "Step:  67900 Current evidence: -123.25\n",
      "Step:  68000 Current evidence: -131.15\n",
      "Step:  68100 Current evidence: -138.18\n",
      "Step:  68200 Current evidence: -132.91\n",
      "Step:  68300 Current evidence: -137.60\n",
      "Step:  68400 Current evidence: -135.08\n",
      "Step:  68500 Current evidence: -137.91\n",
      "Step:  68600 Current evidence: -131.86\n",
      "Step:  68700 Current evidence: -134.29\n",
      "Step:  68800 Current evidence: -136.17\n",
      "Step:  68900 Current evidence: -136.82\n",
      "Step:  69000 Current evidence: -129.86\n",
      "Step:  69100 Current evidence: -129.29\n",
      "Step:  69200 Current evidence: -127.01\n",
      "Step:  69300 Current evidence: -137.57\n",
      "Step:  69400 Current evidence: -129.71\n",
      "Step:  69500 Current evidence: -129.90\n",
      "Step:  69600 Current evidence: -132.97\n",
      "Step:  69700 Current evidence: -133.64\n",
      "Step:  69800 Current evidence: -129.81\n",
      "Step:  69900 Current evidence: -124.42\n",
      "Step:  70000 Current evidence: -132.95\n",
      "Step:  70100 Current evidence: -137.74\n",
      "Step:  70200 Current evidence: -129.04\n",
      "Step:  70300 Current evidence: -138.57\n",
      "Step:  70400 Current evidence: -128.86\n",
      "Step:  70500 Current evidence: -130.87\n",
      "Step:  70600 Current evidence: -126.55\n",
      "Step:  70700 Current evidence: -134.67\n",
      "Step:  70800 Current evidence: -137.69\n",
      "Step:  70900 Current evidence: -136.50\n",
      "Step:  71000 Current evidence: -136.29\n",
      "Step:  71100 Current evidence: -130.59\n",
      "Step:  71200 Current evidence: -137.79\n",
      "Step:  71300 Current evidence: -147.49\n",
      "Step:  71400 Current evidence: -128.58\n",
      "Step:  71500 Current evidence: -131.05\n",
      "Step:  71600 Current evidence: -133.57\n",
      "Step:  71700 Current evidence: -127.82\n",
      "Step:  71800 Current evidence: -135.77\n",
      "Step:  71900 Current evidence: -131.21\n",
      "Step:  72000 Current evidence: -137.18\n",
      "Step:  72100 Current evidence: -133.85\n",
      "Step:  72200 Current evidence: -129.28\n",
      "Step:  72300 Current evidence: -131.02\n",
      "Step:  72400 Current evidence: -133.04\n",
      "Step:  72500 Current evidence: -138.43\n",
      "Step:  72600 Current evidence: -122.52\n",
      "Step:  72700 Current evidence: -135.27\n",
      "Step:  72800 Current evidence: -127.55\n",
      "Step:  72900 Current evidence: -133.75\n",
      "Step:  73000 Current evidence: -135.00\n",
      "Step:  73100 Current evidence: -134.48\n",
      "Step:  73200 Current evidence: -133.40\n",
      "Step:  73300 Current evidence: -136.30\n",
      "Step:  73400 Current evidence: -130.95\n",
      "Step:  73500 Current evidence: -130.47\n",
      "Step:  73600 Current evidence: -131.03\n",
      "Step:  73700 Current evidence: -128.89\n",
      "Step:  73800 Current evidence: -133.11\n",
      "Step:  73900 Current evidence: -146.68\n",
      "Step:  74000 Current evidence: -128.78\n",
      "Step:  74100 Current evidence: -133.95\n",
      "Step:  74200 Current evidence: -127.69\n",
      "Step:  74300 Current evidence: -132.71\n",
      "Step:  74400 Current evidence: -134.25\n",
      "Step:  74500 Current evidence: -131.57\n",
      "Step:  74600 Current evidence: -124.95\n",
      "Step:  74700 Current evidence: -129.40\n",
      "Step:  74800 Current evidence: -135.44\n",
      "Step:  74900 Current evidence: -135.26\n",
      "Step:  75000 Current evidence: -131.14\n",
      "Step:  75100 Current evidence: -130.79\n",
      "Step:  75200 Current evidence: -130.09\n",
      "Step:  75300 Current evidence: -132.78\n",
      "Step:  75400 Current evidence: -142.09\n",
      "Step:  75500 Current evidence: -133.59\n",
      "Step:  75600 Current evidence: -138.13\n",
      "Step:  75700 Current evidence: -139.18\n",
      "Step:  75800 Current evidence: -131.04\n",
      "Step:  75900 Current evidence: -129.04\n",
      "Step:  76000 Current evidence: -132.08\n",
      "Step:  76100 Current evidence: -129.59\n",
      "Step:  76200 Current evidence: -125.49\n",
      "Step:  76300 Current evidence: -131.16\n",
      "Step:  76400 Current evidence: -136.23\n",
      "Step:  76500 Current evidence: -140.08\n",
      "Step:  76600 Current evidence: -131.03\n",
      "Step:  76700 Current evidence: -131.92\n",
      "Step:  76800 Current evidence: -127.97\n",
      "Step:  76900 Current evidence: -130.93\n",
      "Step:  77000 Current evidence: -131.40\n",
      "Step:  77100 Current evidence: -133.02\n",
      "Step:  77200 Current evidence: -125.72\n",
      "Step:  77300 Current evidence: -127.36\n",
      "Step:  77400 Current evidence: -127.03\n",
      "Step:  77500 Current evidence: -133.95\n",
      "Step:  77600 Current evidence: -139.75\n",
      "Step:  77700 Current evidence: -129.79\n",
      "Step:  77800 Current evidence: -132.15\n",
      "Step:  77900 Current evidence: -136.81\n",
      "Step:  78000 Current evidence: -133.19\n",
      "Step:  78100 Current evidence: -135.81\n",
      "Step:  78200 Current evidence: -130.94\n",
      "Step:  78300 Current evidence: -125.32\n",
      "Step:  78400 Current evidence: -120.73\n",
      "The evidence is: -27956.50\n"
     ]
    }
   ],
   "source": [
    "l = np.ones((21,7)) * 100.\n",
    "sigma = np.ones((7,)) * 1.\n",
    "c = np.ones((7,)) * 1e-6\n",
    "\n",
    "kernel = White(c=c, dim_in=21, dim_out=7) + Matern(l=l, sigma=sigma, mu=2.5, dim_in=21, dim_out=7)\n",
    "gpr = ExactGPR(kernel=kernel)\n",
    "gpr.fit(X_train_, Y_train_, call_hyper_opt=True, solver=\"Adam\", adam_tolerance=1e-5, adam_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.9547, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pred = gpr.predict(X_test)\n",
    "print(Loss(pred, Y_test))\n",
    "\n",
    "del gpr, pred, kernel\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titsias' variational EM GPR: m=250, 2.8788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current mean elbo: -718055.41:  85%| | 17/20 [07:03<01:21, 27.27s/it]"
     ]
    }
   ],
   "source": [
    "from aslearn.nonparametric.vigpr import VariationalEMSparseGPR\n",
    "from aslearn.kernel.kernels import White, RBF\n",
    "\n",
    "l = np.ones([21, 1]) * 1.\n",
    "sigma = np.ones([1,]) * 1.\n",
    "c = np.ones([1,]) * 0.1\n",
    "\n",
    "white_kernel = White(c=c, dim_in=21, dim_out=1)\n",
    "kernel = RBF(sigma=sigma, l=l, dim_in=21, dim_out=1)\n",
    "\n",
    "gpr = VariationalEMSparseGPR(kernel=kernel, white_kernle=white_kernel)\n",
    "ind = gpr.fit(X_train, Y_train, m=20, subsetNum=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.2787, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = gpr.predict(X_test)\n",
    "print(Loss(pred, Y_test))\n",
    "\n",
    "del gpr, pred, kernel\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abb",
   "language": "python",
   "name": "abb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
