{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The benchmark of SARCOS dataset\n",
    "\n",
    "The performance of different algorithms on this dataset can be found in: https://github.com/Kaixhin/SARCOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with fourier(polynomial()) feature: 2.111,  \n",
    "NN feature not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jylearn.data.reg_data import robot_inv_data\n",
    "import numpy as np\n",
    "import torch as th\n",
    "device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "\n",
    "Loss = th.nn.MSELoss()\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = robot_inv_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature dim:  6805\n"
     ]
    }
   ],
   "source": [
    "X, Y = np.concatenate([X_train, X_val], axis=0), np.concatenate([Y_train, Y_val], axis=0)\n",
    "from jylearn.feature.polynomial import PolynomialFT\n",
    "from jylearn.feature.bellcurve import BellCurve\n",
    "from jylearn.feature.fourier import FourierBases\n",
    "from jylearn.parametric.ridge import RidgeReg\n",
    "\n",
    "param1 = np.linspace(0.05, 0.99, 7)\n",
    "param2 = np.arange(1, 7)\n",
    "\n",
    "f1 = FourierBases(param1, param2)\n",
    "f2 = PolynomialFT(2)\n",
    "\n",
    "X_f = f1(f2(X))\n",
    "X_test_f = f1(f2(X_test))\n",
    "print(\"feature dim: \", X_f.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c5e2e2be41da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_f_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_f_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_t_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidgeReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_f_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_f_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_t_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_f' is not defined"
     ]
    }
   ],
   "source": [
    "X_f_t, Y_t, X_test_f_t, Y_t_test = th.from_numpy(X_f).to(device), th.from_numpy(Y).to(device), th.from_numpy(X_test_f).to(device), th.from_numpy(Y_test).to(device)\n",
    "rr = RidgeReg().fit(X_f_t, Y_t)\n",
    "pred = rr.predict(X_test_f_t)\n",
    "mse = Loss(pred, Y_t_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test =\\\n",
    "    th.from_numpy(X_train).to(device), th.from_numpy(Y_train).to(device), th.from_numpy(X_val).to(device), \\\n",
    "    th.from_numpy(Y_val).to(device), th.from_numpy(X_test).to(device), th.from_numpy(Y_test).to(device)\n",
    "    \n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from jylearn.parametric.mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network: 1.469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=21, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=500, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "Curr validation loss:  tensor(22.8014, device='cuda:0')\n",
      "Curr validation loss:  tensor(19.0006, device='cuda:0')\n",
      "Curr validation loss:  tensor(16.6982, device='cuda:0')\n",
      "Curr validation loss:  tensor(13.5147, device='cuda:0')\n",
      "Curr validation loss:  tensor(13.6710, device='cuda:0')\n",
      "Curr validation loss:  tensor(10.9160, device='cuda:0')\n",
      "Curr validation loss:  tensor(10.5393, device='cuda:0')\n",
      "Curr validation loss:  tensor(10.2446, device='cuda:0')\n",
      "Curr validation loss:  tensor(9.8874, device='cuda:0')\n",
      "Curr validation loss:  tensor(9.7807, device='cuda:0')\n",
      "Curr validation loss:  tensor(9.4553, device='cuda:0')\n",
      "Curr validation loss:  tensor(8.9885, device='cuda:0')\n",
      "Curr validation loss:  tensor(8.5249, device='cuda:0')\n",
      "Curr validation loss:  tensor(9.2376, device='cuda:0')\n",
      "Curr validation loss:  tensor(9.1354, device='cuda:0')\n",
      "Curr validation loss:  tensor(8.6088, device='cuda:0')\n",
      "Curr validation loss:  tensor(8.5999, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.9240, device='cuda:0')\n",
      "Curr validation loss:  tensor(8.6330, device='cuda:0')\n",
      "Curr validation loss:  tensor(8.4300, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.8112, device='cuda:0')\n",
      "Curr validation loss:  tensor(8.3339, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.8098, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.7532, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.5085, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.3873, device='cuda:0')\n",
      "Curr validation loss:  tensor(8.1617, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.8857, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.5065, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.4419, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.6030, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.8353, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.3972, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.5307, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.8178, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.3449, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.5083, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.4069, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.5158, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.4773, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.3404, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.8530, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.8128, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.1676, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.1505, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.1532, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.1066, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.8396, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.4555, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.8227, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.2564, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.4174, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.4307, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.0498, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.1755, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.0298, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.3282, device='cuda:0')\n",
      "Curr validation loss:  tensor(8.2088, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.3321, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.9436, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.0588, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.3675, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.1844, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.4472, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.2262, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.3934, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.2455, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.8465, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.7772, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.1430, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.7591, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.0699, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.9920, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.8468, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.1069, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.9995, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.6254, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.0444, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.0926, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.1906, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.0040, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.8295, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.0819, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.4780, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.3896, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.8860, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.8874, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.7876, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.6803, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.9875, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.9031, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.0117, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.2953, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.8006, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.1255, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.7557, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.7179, device='cuda:0')\n",
      "Curr validation loss:  tensor(7.0827, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.6788, device='cuda:0')\n",
      "Curr validation loss:  tensor(6.6788, device='cuda:0')\n",
      "tensor(1.4686, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "param = {\"layer\":4, \"nodes\":[21, 500, 500, 7], \"batch\":128, \"lr\":1e-3, \"decay\":0.}\n",
    "net = MLP(param).to(device)\n",
    "print(net)\n",
    "Loss = MSELoss()\n",
    "parameters = MLP.setParams(net, param[\"decay\"])\n",
    "optimizer = Adam(parameters, lr=param[\"lr\"])\n",
    "for _ in range(100):\n",
    "    for i in range(len(X_train)//param[\"batch\"]):\n",
    "        optimizer.zero_grad()\n",
    "        index = th.randperm(len(X_train))\n",
    "        curr_index = index[i*param[\"batch\"]:(i+1)*param[\"batch\"]]\n",
    "        X_b = X_train[curr_index]\n",
    "        Y_b = Y_train[curr_index]\n",
    "        pred = net(X_b)\n",
    "        L = Loss(pred, Y_b)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    with th.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        L_val = Loss(pred_val, Y_val)\n",
    "        print(\"Curr validation loss: \", L_val)\n",
    "net.eval()\n",
    "pred_test = net(X_test)\n",
    "print(Loss(pred_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsample GPR without hyperparameters optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jylearn.nonparametric.gpr import ExactGPR\n",
    "from jylearn.kernel.kernels import RBF, White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = X_test.double(), Y_test.double()\n",
    "perm = th.randperm(X_train.size(0))\n",
    "idx = perm[:10000]\n",
    "X_train_, Y_train_ = X_train[idx].double(), Y_train[idx].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 924.00 MiB (GPU 0; 7.80 GiB total capacity; 4.97 GiB already allocated; 798.12 MiB free; 5.44 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-779651751fa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRBF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mWhite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExactGPR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/MY_ML/jylearn/nonparametric/gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, call_hyper_opt)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (n,n_y) yd-output space dim, important -> cho_solve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 924.00 MiB (GPU 0; 7.80 GiB total capacity; 4.97 GiB already allocated; 798.12 MiB free; 5.44 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "l = np.ones(21,) * 40\n",
    "kernel = RBF(l=l, dim=21, sigma=1.) + White(dim=21, c=0.002)\n",
    "gpr = ExactGPR(kernel=kernel)\n",
    "gpr.fit(X_train_, Y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4927, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "pred = gpr.predict(X_test)\n",
    "print(Loss(pred, Y_test))\n",
    "del gpr, pred, kernel\n",
    "\n",
    "th.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
